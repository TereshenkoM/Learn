В Postgres есть оптимизатор запросов, который строит планы на выполнение запросов и сравнивает их по стоимости. Выполняется тот, который занимает меньше времени.

Стоимость запроса учитывает кол-в IO операций и кол-во процессорного времени на обработку данных. Также на стоимость влияет процессорная обработка.

Как он это понимает? Он использует и хранит статистику по данным. 

Пример
Есть таблица и два запроса
```postgresql
\d employee

 Таблица "public.employee"
   Столбец | Тип | Правило сортировки | Допустимость NULL | По умолчанию
-------------+-----------------------+--------------------+-------------------+
 employee_id | bigint | | not null | generated always as identity
 name | character varying(50) | | not null |
 department | character varying(10) | | |
Индексы:
    "employee_pkey" PRIMARY KEY, btree (employee_id)
    "employee_department_idx" btree (department )

explain analyze
select * from employee
where department='accounting';
-- bitmap index scan, overall cost up to 134610.23

explain analyze
select * from employee
where department='IT';
-- seq scan, overall cost up to 215001
```

В первом запросе будет использоваться `bitmap index scan`, а во втором `seq scan`. Несмотря на то, что у нас есть индекс на колонке `department`, этот индекс будет использован только в первом запросе, но не во втором. Почему так?

Потому что постгрес знает, какие данные лежат в таблице.
```postgresql
\x

select schemaname, tablename, most_common_vals, most_common_freqs
from pg_stats
where tablename = 'employee' and attname = 'department';

-----------------------------------------------------------
schemaname        | public
tablename         | employee
most_common_vals  | {sales,IT,management,accounting}
most_common_freqs | {0.5941,0.34493333,0.0502,0.010766666}
```

Postrgres знает, какие наиболее частые значения есть в таблица `employee` в колонке `department`. Знает, что:
- `sales` – 59.41% записей
- `IT` – 34.49% записей
- `management` – 5.02% записей
- `accounting` – 1.08% записей

Как видим accouting - это меньше 1% записей, следовательно селективность высокая и индекс может быть использована.

**Мы приходим к тому, что нужно знать сами данные в БД чтобы понять, будет ли использован индекс или нет.**

Соответственно, если статистики нет или она неточная, то PostgreSQL не может строить правильные планы выполнения запросов. Поэтому после массового изменения данных можно запускать `ANALYZE`, чтобы принудительно собрать статистику. Например:
```postgresql
analyze employee;
```

В Postgres есть возможность просмотреть, когда была собрана аналитика
```postgresql
select relname, last_analyze, last_autoanalyze
from pg_stat_all_tables
where relname = 'employee';

 relname  | last_analyze                  | last_autoanalyze
----------+-------------------------------+------------------
 employee | 2025-02-25 19:37:30.082675+03 |
(1 строка)
```

Если давно или до нашего какого-то массового изменения данных — можно просто перезапустить сбор аналитики вручную с `ANALYZE`.

Ну и иногда можно уточнять статистику. По умолчанию анализатор берет 300 рандомных, то есть случайных значений из таблицы и на их основе составляет статистику. Если таблица большая и данные распределены очень неравномерно, то можно уточнять сбор статистики по этой колонке вот так:


```postgresql
alter table employee alter column department set statistics 1000;
analyze employee;
```

Теперь будет браться не 300 строк, а 1000 строк для анализа. Конечно, теперь дольше будет выполняться анализ и больше значений надо будет хранить в статистике, но иногда это даёт планировщику запросов больше точных данных о таблице и он начинает в каких-то отдельных случаях строить лучшие планы. Наобум уточнять везде статистику не надо, конечно. Но если вы видите, что PostgresSQL неверно оценивает количество строк в каком-то месте, но статистика собрана недавно, то для большой таблицы можно попробовать уточнить в ней статистику по нужному столбцу.