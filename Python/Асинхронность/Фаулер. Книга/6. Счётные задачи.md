Вопросы:
* Библиотека multiprocessing
* Создание пулов процессов для счётных задач
* Использования async и await для управления счётными задачами
* Использование MapReduce для решения счётной задачи с помощью asyncio
* Разделение данных между процессами с применением блокировок
* Повышение скорости работы программ, содержащих операции, ограниченные быстродействием процессора и производительностью ввода-вывода


Ранее мы говорили о том, какой выигрыш позволяет получить asyncio при конкурентном выполнении операций ввода-вывода. Ранее мы чётко следили, чтобы в программах не было счётных операций. На первый взгляд это серьёзное ограничение, но на деле asycnio более универсальна.

В asycnio имеется API для взаимодействия с библиотекой multiprocessing. Это позволяет использоваться предложение await и другие API для работы с несколькими процессами и таким образом получать выигрыш в производительности **даже при выполнении счётного кода. При этом мы обходим GIL**. 

## Введение в библиотеку multiprocessing

GIL препятствует параллельному выполнению нескольких участков байт-кода. Это означает, что для любых задач, кроме ввода-вывода, многопоточность не даёт никакого выигрыша в производительности.


В multiprocessing вместо запуска потоков для распараллеливания работы родительский процесс будет запускать дочерние процессы. В каждом дочернем процессе работает отдельный интерпретатор Python со своим GIL. **Если код выполняется на машине с несколькими ядрами, то означает, что можно эффективно распараллелить задачи. И даже если процессов больше, чем ядер, механизм вытесняющей многозадачности, встроенный в ОС, позволит выполнить задачи конкурентно**.

Такая конфигурация является конкурентной и параллельной.

Пример
```python
# Два параллельных процесса
import time
from multiprocessing import Process
  

def count(count_to: int) -> int:
	start = time.time()
	counter = 0

	while counter < count_to:
		counter += 1

	end = time.time()
	print(f'Закончен подсчёт до {count_to} за время {end-start}')

	return counter


if __name__ == "__main__":
	start_time = time.time()

	# Создаём процессы для выполнения
	to_one_hundred_million = Process(target=count, args=(100000000,))
	to_two_hundred_million = Process(target=count, args=(200000000,))

	# Запускаем процесс. Управление возвращается немедленно
	to_one_hundred_million.start()
	to_two_hundred_million.start()

	# Ждём завершения процесса. Этот метод блокирует управление, пока процесс не завершится

	to_one_hundred_million.join()
	to_two_hundred_million.join()

	end_time = time.time()

	print(f'Полное времпя работы {end_time-start_time}')
```

Вывод будет примерно таким
```python
Закончен подсчет до 100000000 за время 5.3844
Закончен подсчет до 200000000 за время 10.6265
Полное время работы 10.8586
```

Обе функции заняли ~16 секунд. А весь код завершился примерно за ~11 секунд.

**Примечание**
Особенность библиотеку multiprocessing заключается в том, что если запустить всё вне if main, то мы рискуем получить ошибку `An attempt has been made to start a new process before the curreny process has finished its bootstrapping phase» (Попытка запустить новый процесс до завершения фазы инициализации текущего процесса)`
Нужно это для того, чтоб помешать другим программам, импортирующим наш код, случайно запустить неколько процессов.


Преимущество мы получили, но постоянные start и join выглядят не элегантно. Кроме того, мы не знаем, какой процесс закончится первым (если собираемся использовать что-то вроде asyncio.as_completed). 
**Метод join не возвращает то значение, которое вернула выполненная функция. На самом деле НЕТ способа получить возвращаемое значение без использования разделяемой памяти!**

Такой API подходит только для простых случаев.


## Использование пулов процессов

Пул процессов напоминает пул подключений, который мы видели в главе 5.

**Разница  в том, что мы создаём не набор подключений к БД, а набор Python-процессов, который можно использовать для параллельного выполнения функций.**

За кулисами он выполняет функцию в доступном процессе и возвращает её значение.

Пример
```python
# Создание пула процессов
from multiprocessing import Pool


def say_hello(name: str) -> str:
	return f'Привет, {name}'


if __name__ == "__main__":
	# Создали пулл
	with Pool() as process_pool:
		# Выполнить say_hello в отдельных процессах
		hi_jeff = process_pool.apply(say_hello, args=('Jeff',))
		hi_john = process_pool.apply(say_hello, args=('John',))

		print(hi_jeff)
		print(hi_john)
```

Здесь мы создаём пул процессов в предложении with Pool() as process_pool. Т.к. это контекстный менеджер, после завершения работы можно корректно остановить созданные Python-процессы. Если этого не сделать, то возникает риск утечки.

При создании пула автоматически создаётся столько процессов, сколько имеется процессорных ядер. Для получения количества ядер использутися `multiprocessing.cpu_count()`. Если это не годится, то можно передать в Pool произвольное целое число в аргументе processes.

**Примечание**
Метод apply блокирует выполнение, пока функция не завершится.


### Асинхронное получение результатов
Метод apply блокирует выполнение, пока функция не завершится. Обойти это можно при помощи apply_async. Он сразу же возвращает AsyncResult и начинает выполнять процесс в фоновом режиме. Имея этот объект мы можем вызвать метод get и получить результат.
```python
# Асинх. получение результатов от пула процессов
from multiprocessing import Pool


def say_hello(name: str) -> str:
	return f'Привет, {name}'


if __name__ == "__main__":
	with Pool() as process_pool:
		hi_jeff = process_pool.apply_async(say_hello, args=("Jeff",))
		hi_john = process_pool.apply_async(say_hello, args=("John",))

		print(hi_jeff.get())
		print(hi_john.get())
```

В этом случае метод get блокирует родительский поток.
Однако сразу отреагировать не получится, если hI_jeff занимает 10 секунд, а hi_john занимает 1, то программа зависнет на 10 секунд. 

Чтобы решить эту проблему используют исполнитель пула процессов в сочетании с asycnio.


## Использование исполнителей пула процессов в сочетании с asyncio

Библиотека concurent.futures - содержит исполнители для процессов и потоков, которые можно использовать как самостоятельно, так и в сочетании с asycnio. 

## Введение в исполнители пулов процессов
API пула процессов тесно связан с процессами, но многопроцессность - один из способов реализовать вытесняющую многозадачность. Другим является многопоточность.

class Executor - абстракция передачи работы пулу процессов,которой всё равно, что такое ресурс: процесс, поток или ещё что-то. В нём определены два метода:
* submit - принимает вызываемый объект и возвращает объект Future (это не одно и то же, что объект future из asyncio)
* map -принимает вызываемый объект и список аргументов, после чего асинхронно выполняет объект с каждым из этих аргументов. Возвращается итератор по результатам вызовов. Это аналогично as_completed в том смысле, что результаты становятся доступны по мере их поступления. 

У класса есть две реализации:
* ProcessPoolExecutor (так как реализуем процессы, то сосредоточимся на нём)
* ThreadPoolExecutor


Пример
```python
# Исполнители пула процессов
import time
from concurrent.futures import ProcessPoolExecutor


def count(count_to: int) -> int:
	start = time.time()
	counter = 0

	while counter < count_to:
		counter += 1

	end = time.time()
	print(f'Закончен подсчёт до {count_to} за время {end - start}')

	return counter


if __name__ == "__main__":
	with ProcessPoolExecutor() as process_pool:
		numbers = [1, 3, 5, 22, 100000000]
		for result in process_pool.map(count, numbers):
			print(result)
```

Вывод
```python
Завершен подсчет до 1 за время 9.5367e-07
Завершен подсчет до 3 за время 9.5367e-07
Завершен подсчет до 5 за время 9.5367e-07
Завершен подсчет до 22 за время 3.0994e-06
1
3
5
22
Завершен подсчет до 100000000 за время 5.2097
100000000
```

Порядок итераций детерминирован и определяется тем,кв каком порядке следуют числа в списке numbers. Т.е. если бы первым числом было 100000000, то пришлось бы ждать её выполнения. Эта техника не такая отзывчивая как asyncio.as_completed().

### Исполнители пула процессов в сочетании с циклом событий

Познакомившись с тем, как работают исполнители пула процессов, посмотрим как включить их в цикл событий asyncio.

Исполнитель пула процессов создаётся так же, как было описано выше, т.е. в контекстном менеджере. **Имея пул мы можем использовать специальный метод цикла событий asyncio - run_in_executor.** Этот метод принимает выполняемый объект и исполнитель (пула процессов или пула потоков), после чего исполняет  этот объект внутри пула и возвращает допускающий ожидание объект, который можно использовать в предложении await или передать какой-нибудь функции API, например gather.

Метод run_in_executor не позволяет передавать аргументы, потому воспользуемся partial.

```python
# Исполнитель пула процессов в сочетании с asycnio
import asyncio
from asyncio.events import AbstractEventLoop
from concurrent.futures import ProcessPoolExecutor
from functools import partial
from typing import List


def count(count_to: int) -> int:
	counter = 0

	while counter < count_to:
		counter += 1

return counter
  

async def main():
	with ProcessPoolExecutor() as process_pool:
		# Получаем цикл событий
		loop: AbstractEventLoop = asyncio.get_running_loop()
		nums = [1, 3, 5, 22, 100000000]
		# Создаём список с частично применяемыми функциями (partial)
		calls: List[partial[int]] = [partial(count, num) for num in nums]

		call_coros = []
		for call in calls:
			call_coros.append(loop.run_in_executor(process_pool, call))
		# Ждать получения результатов
		results = await asyncio.gather(*call_coros)

		for result in results:
			print(result)


if __name__ == "__main__":
	asyncio.run(main())
```

Здесь же можно было использовать as_completed для получения результат по мере поступления (порядок выполнения при это недетерменирован, то есть не зависел от порядка элементов в списке).

**Примечание**
Как взаимодействуют процессы и цикл событий
- **Процессы**:
    - Каждый вызов `count` выполняется в отдельном процессе. Это означает, что если у вас несколько ядер CPU, они могут быть использованы для параллельного выполнения задач.
        
    - Каждый процесс работает независимо и не зависит от цикла событий.
    
- **Цикл событий**:
    - Цикл событий управляет асинхронными задачами. В вашем случае, он используется для запуска задач в пуле процессов и ожидания их завершения.
    
    - Цикл событий работает в основном потоке и координирует выполнение асинхронных операций.

## Решение задачи с помощью MapReduce и asyncio

В случае, когда необходимо обработать большой набор данных, используется операция MapReduce.

Большой набор данных разбивается на части, затем мы можем решить задачу для поднабора данных - эта операция называется отображением (mapping). 
После того, как все задачи поднабора решены, мы можем объединить результаты в окончательный ответ. Этот шаг называется редуцией (reducing) (т.е. сводим несколько ответов в один).

![[Pasted image 20250206162451.png]]


Пример (однопоточный, подсчитываем, сколько раз в наборе слов встречается каждое слово)

```python
# Однопоточная модель MapReduce
import functools
from typing import Dict


def map_frequency(text: str) -> Dict[str, int]:
	words = text.split(' ')
	frequencies = {} 

	for word in words:
		if word in frequencies:
			frequencies[word] = frequencies[word] + 1
		else:
			frequencies[word] = 1

	return frequencies


def merge_dictionaries(first: Dict[str, int], second: Dict[str, int]) -> Dict[str, int]:
	merged = first

	for key in second:
		if key in merged:
			merged[key] += second[key]
		else:
			merged[key] = second[key]

	return merged

lines = [
	"I know what I know",
	"I know that I know",
	"I dont know much",
	"They dont know much"
]

mapped_results = [map_frequency(line) for line in lines]

for result in mapped_results:
	print(result)

print(functools.reduce(merge_dictionaries, mapped_results))
```

### Применение asycnio для отображения и редуции

Для примера будем использовать набор n-грамм от Google.  Для примера будем искать: "Сколько раз в литературе встречалось слово Aardvark начиная с 1500 года".

Пример (данный скрипт работает долго)
```python
# Подсчёт частот слов, начинающихся на букву a
import time

freqs = {}

  

with open(
	'chapter_6/googlebooks-eng-all-1gram-20120701-a', encoding='utf-8'
) as f:
	lines = f.readlines()

	start = time.time()
	
	for line in lines:
		data = line.split('\t')
		word = data[0]
		count = int(data[2])
	
		if word in freqs:
			freqs[word] += count
		else:
			freqs[word] = count
	
	end = time.time()
	print(f'{end-start:.4f}')
	```

Как улучшить:
1. Разбить набор данных на меньшие порции (при помощи генератора)
2. Создать функцию отображения, которая затем будет запущено параллельно

Пример (улучшенный скрипт)
```python
# Распараллеливание с помощью MapReduce и пула процессов
import asyncio
import concurrent.futures
import functools
import time
from typing import Dict, List


def partition(data: List, chunk_size: int):
	for i in range(0, len(data), chunk_size):
		yield data[i:i+chunk_size]

  
  

def map_frequencies(chunk: List[str]) -> Dict[str, int]:
	counter = {}

	for line in chunk:
		word, _, count, _ = line.split('\t')
		if counter.get(word):
			counter[word] += int(count)
		else:
			counter[word] = int(count)

	return counter


def merge_dictionaries(first: Dict[str, int], second: Dict[str, int]) -> Dict[str, int]:
	merged = first

	for key in second:
		if key in merged:
			merged[key] += second[key]
		else:
			merged[key] = second[key]

	return merged


async def main(partition_size: int):
	with open('chapter_6/googlebooks-eng-all-1gram-20120701-a', encoding='utf-8') as f:
		contents = f.readlines()

		loop = asyncio.get_running_loop()
		tasks = []

		start = time.time()

		with concurrent.futures.ProcessPoolExecutor() as pool:
			for chunk in partition(contents, partition_size):
				tasks.append(loop.run_in_executor(
					pool,
					functools.partial(map_frequencies, chunk)
				))

			intemediate_results = await asyncio.gather(*tasks)
			final_results = functools.reduce(
				merge_dictionaries,
				intemediate_results
			)
			print(f'Aardvard встречается {final_results} раз')

			end = time.time()

			print(f'Время MapReduce: {end-start:.4f} секунд')

if __name__ == '__main__':
	asyncio.run(main(60000))
```

Однако тут всё ещё остался счётный код, допускающий распараллеливания. Операция редуции.

Пример (функции отображения, объединения и разбиения опущены)
```python
# Распараллеливание операции reduce
import asyncio
import concurrent.futures
import functools
import time
from typing import Dict, List
from chapter_6.listing_6_8 import partition, merge_dictionaries, map_frequencies


async def reduce(loop, pool, counters, chunk_size) -> Dict[str, int]:
	# Разбить словари на допускающие ожидание порции
	chunks: List[List[Dict]] = list(partition(counters, chunk_size))
	reducers = []

	while len(chunks[0]) > 1:
		for chunk in chunks:
			reducer = functools.partial(
				functools.reduce,
				merge_dictionaries,
				chunk
			)
			reducers.append(reducer)

		reducer_chunks = await asyncio.gather(*reducer)
	chunks = list(partition(reducer_chunks, chunk_size))
	reducers.clear()

	return chunks[0][0]


async def main(partition_size: int):
	with open('chapter_6/googlebooks-eng-all-1gram-20120701-a', encoding='utf-8') as f:
	contents = f.readlines()
	loop = asyncio.get_event_loop()

	tasks = []
	with concurrent.futures.ProcessPoolExecutor() as pool:
		start = time.time()

		for chunk in partition(contents, partition_size):
			tasks.append(loop.run_in_executor(
				pool, functools.partial(map_frequencies, chunk)
			))
	
		intermediate_results = await asyncio.gather(*tasks)
		final_result = await reduce(loop, pool, intermediate_results, 500)

		print(f'Aardvark has appeared {final_result['Aadvark']} times.')

		end = time.time()

		print(f'Время MapReduce: {(end - start):.4f} секунд')


if __name__ == "__main__":
	asyncio.run(main())
```


## Разделяемые данные и блокировки

Каждый процесс имеет собственную память. Изолированную от других процессов. Однако в библиотеке multiprocessing есть так называемые **объекты разделяемой памяти**. Он выделен так, чтобы к нему могли обращаться разные процессы
![[Pasted image 20250206164221.png]]

Если работа с ней организована некорректно, то возможны ошибки, которые трудно воспроизвести. Вообще лучше избегать разделяемого состояния, но иногда без него не обойтись.

### Разделяемые данные и состояние гонки

Библиотека multiprocessing поддерживает два вида разделяемых данных:
* Значение - одиночное, вроде целого числа или числа с плав. точкой
* Массив - массив одиночных значений

В разделяемой памяти можно хранить только данные тех типов, которые определены в модуле array.

Пример
```python
from multiprocessing import Process, Value, Array


def increment_value(shared_int: Value):
	shared_int.value += 1


def increment_array(shared_array: Array):
	for index, integer, in enumerate(shared_array):
		shared_array[index] = integer + 1
  

if __name__ == "__main__":
	integer = Value('i', 0)
	integer_array = Array('i', [0,0])

	procs = [
		Process(target=increment_value, args=(integer,)),
		Process(target=increment_array, args=(integer_array,))
	]

	[p.start() for p in procs]
	[p.join() for p in procs]

	print(integer.value)
	print(integer_array[:])
```

Этот код работает хорошо, но что если будет модифицироваться одно и то же значение в разных потоках?
Как например здесь
```python
from multiprocessing import Process, Value
  

def increment_value(shared_int: Value):
	shared_int.value += 1


if __name__ == "__main__":
	for _ in range(100):
		integer = Value('i', 0)

		procs = [
			Process(target=increment_value, args=(integer,)),
			Process(target=increment_value, args=(integer,))
		]

		[p.start() for p in procs]
		[p.join() for p in procs]

		print(integer.value)
		assert(integer.value == 2)
```

В таком случае изредка может возникать ошибка (ошибка недетерминированная) 
```python
2
2
2
Traceback (most recent call last):
File "listing_6_11.py", line 17, in <module>
assert(integer.value == 2)
AssertionError
1
```

Иногда счётчик становится равен 1. Это происходит из-за состояния гонки. Оно возникает если исход последовательности операций зависит от того,какая из них закончится первой.

Из-за того, что для увеличения значения на единицу его нужно сначала прочитать, прибавить 1, а потом записать в память. Разделямое значение зависит от того, в какой момент он (процесс) его читает.
![[Pasted image 20250206165709.png]]
Здесь состояния гонки удалось избежать. Процесс 1 увеличивает значение, перед тем как Процесс 2 читает его, а потому выигрывает гонку. Но что если в гонке случилась ничья?

![[Pasted image 20250206165841.png]]

Здесь оба процесса читают начальное значение, равное 0. Затем оба увеличивают его, получают 1 и одновременно записывают в память. Здесь проблема в том, что инкрементирование состоит из двух операций, что и является причиной проблемы, - операция называется неатомарной или потоконебезопастной. Предвидеть это не так уж просто.

### Синхронизация при помощи блокировок

Избежать гонки можно, синхронизировав доступ к тем разделяемым данным, которые собрались модифицировать. (То есть чтобы операции гарантированно завершались в определённом порядке).

Один из механизмов для синхронизации доступа к разделяемым данным называется блокировкой или мьютексом. Он позволяет одному процессу заблокировать участок кода, т.е. запретить всем остальным его выполнение. Такой участок называется **критической секцией**.

Блокировки поддерживают две основные операции:
* Захват
* Освобождение

Гарантируется, что процесс, захвативший блокировку - единственный, кто может выполнять код в критической секции. Закончив выполнение он освобождает блокировку. 
![[Pasted image 20250206170640.png]]


Пример
```python
# Захват или освобождение блокировки
from multiprocessing import Process, Value


def increment_value(shared_int: Value):
	shared_int.get_lock().acquire()
	shared_int.value += 1
	shared_int.get_lock().release()

  
  

if __name__ == "__main__":
	for _ in range(100):
		integer = Value('i', 0)
		procs = [
			Process(target=increment_value, args=(integer,)),
			Process(target=increment_value, args=(integer,))
		]

[p.start() for p in procs]
[p.join() for p in procs]

print(integer.value)
assert(integer.value == 2)
```

**Примечание**
Блокировки также являются контекстными менеджерами
```python
def increment_value(shared_int: Value):
	with shared_int.get_lock():
		shared_int.value = shared_int.value + 1
```

Гонка устранена. Но дело в том, что теперь этот код последовательный. Недостаток синхронизации потоков. Потому не стоит идти по лёгкому пути и защищать вообще всё. Стоит защищать только критические части.

### Разделение данных в пулах процессов

Чтобы решить проблему с разделением данных, мы должны поместить разделяемый счётчик в глобальную переменную и каким-то образом дать знать об этом процессам-исполнителям. **Для этого предназначены инициализаторы пула процессов - специальные функции, которые вызываются в момент запуска каждого процесса в пуле.** С их помощью мы можем создавать ссылку на разделяемую память, выделенную в момент его создания

```python
from concurrent.futures import ProcessPoolExecutor
import asyncio
from multiprocessing import Value

shared_counter: Value
  

def init(counter: Value):
	global shared_counter
	shared_counter = counter


def increment():
	with shared_counter.get_lock():
		shared_counter.value += 1
  

async def main():
	counter = Value('i', 0)
	# Выполнять init с аргументов counter для каждого процесса
	with ProcessPoolExecutor(initializer=init, initargs=(counter,)) as pool:
		await asyncio.get_running_loop().run_in_executor(pool, increment)

	print(counter.value)


if __name__ == '__main__':
	asyncio.run(main())
```

Теперь применим к нашему приложению с MapReduce. Создадим разделяемый счётчик, который будет увеличиваться на 1 при завершении каждой операции отображения. Также создадим задачу progress_reporter которая будет работать в фоновом режиме и каждую секунду выводить на консоль информацию о том, насколько далеко мы продвинулись.


```python
from concurrent.futures import ProcessPoolExecutor
import functools
import asyncio
from multiprocessing import Value
from typing import List, Dict
from chapter_6.listing_6_8 import partition, merge_dictionaries

map_progress: Value


def init(progress: Value):
	global map_progress
	map_progress = progress
  

def map_frequencies(chunk: List[str]) -> Dict[str, int]:
	counter = {}

	for line in chunk:
		word, _, count, _ = line.split('\t')
		if counter.get(word):
			counter[word] += count
		else:
			counter[word] = int(count)

	with map_progress.get_lock():
		map_progress.value += 1

	return counter


async def progress_reporter(total_partitions: int):
	while map_progress.value < total_partitions:
		print(
			f'Завершено операций отображения:\
			{map_progress.value}/{total_partitions}'
		)

		await asyncio.sleep(1)
  

async def main(partition_size: int):
	global map_progress
	with open('chapter_6/googlebooks-eng-all-1gram-20120701-a', encoding='utf-8') as f:

	contents = f.readlines()
	loop = asyncio.get_running_loop()
	tasks = []
	map_progress = Value('i', 0)

	with ProcessPoolExecutor(initializer=init, initargs=(map_progress,)) as pool:
		total_partitions = len(contents) // partition_size
		reporter = asyncio.create_task(progress_reporter(total_partitions))
		for chunk in partition(contents, partition_size):
			tasks.append(loop.run_in_executor(
				pool, functools.partial(map_frequencies, chunk)
			))
		counters = await asyncio.gather(*tasks)
		await reporter

		final_result = functools.reduce(merge_dictionaries, counters)
		print(f"Aadvarl встречается {final_result['Aadvark']} раз.")


if __name__ == '__main__':
	asyncio.run(main())
```


## Несколько процессов и несколько циклов событий

Многопроцессность полезна в основном для счётных задач, но может дать некоторый буст для операций ввода-вывода.

Для примера, нагрузка на ЦП в листинге 5.8
![[Pasted image 20250206172927.png]]

Почему такая высокая нагрузка? Дело в том, что для обработки результатов от Postgres используется процессор. Если конкурентно отправлено 100000 запросов, но обрабатывать можно только по одному в каждый момент времени, то будет копиться очередь из необработанных результатов.

Однако пропускную способность моно улучшить при помощи многопроцессности. В этом случае каждый процесс имеет собственный поток и собственный интерпретатор Python. Это открывает возможность создать по одному циклу событий в каждом процессе. Это не увеличит пропускную способность, зато увеличит кол-во результатов, обрабатываемых одновременно.
![[Pasted image 20250206173426.png]]

```python
import asyncio
import asyncpg
from utils import async_timed
from typing import List, Dict
from concurrent.futures.process import ProcessPoolExecutor

product_query = \
"""
	SELECT
	p.product_id,
	p.product_name,
	p.brand_id,
	s.sku_id,
	pc.product_color_name,
	ps.product_size_name
	FROM product p
	JOIN sku s ON s.product_id = p.product_id
	JOIN product_color pc ON pc.product_color_id = s.product_color_id
	JOIN product_size ps ON ps.product_size_id = s.product_size_id
	WHERE p.product_id = 100;
"""

async def query_product(pool):
	async with pool.acquire() as conn:
		return await conn.fetchrow(product_query)


@async_timed()
async def query_products_concurrently(pool, queries):
	queries = [query_product(pool) for _ in range(queries)]
	return await asyncio.gather(*queries)


def run_in_new_loop(num_queries: int) -> List[Dict]:
	async def run_queries():
		async with asyncpg.create_pool(
			host='127.0.0.1',
			port=5432,
			user='ishimura',
			password='1111',
			database='asyncio_eshop',
			min_size=6,
			max_size=6
		) as pool: # создать пул с 6 подключениями
			return await query_products_concurrently(pool, num_queries)

	results = [dict(result) for result in asyncio.run(run_queries())]
	
	return results


@async_timed()
async def main():
	loop = asyncio.get_running_loop()
	pool = ProcessPoolExecutor()
	tasks = [loop.run_in_executor(
		pool, run_in_new_loop, 10000
	) for _ in range(5)]

	all_results = await asyncio.gather(*tasks)

	total_queries = sum([len(result) for result in all_results])
	print(f'Извлечено товаров из БД: {total_queries}')


if __name__ == "__main__":
	asyncio.run(main())
```

Мы написали новую функцию: run_in_new_loop. Внутри неё определена сопрограмма run_queries, которая создаёт новый пул подключений и конкурентно выполняет заданное число запросов. Затем мы вызываем её в новом цикле событий. 



## Резюме

 * Узнали, как выполнять несколько функций Python параллельно с помощью пула процессов
 * Научились создавать исполнитель пула процессов и запускать функции Python параллельно. 
 * Исполнитель пула процессов позволяет использовать такие API asyncio как gather и as_completed
 * Решили задачи MapReduce при помощи пула процессов и asyncio
 * Узнали как разделять между процессами информацию о состоянии
 * Познакомились с состоянием гонки и научились избегать её
 * Узнали как использовать multiprocessing для расширения возможностей asyncio путём создания отдельного цикла событий в каждом процессе.