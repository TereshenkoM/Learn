Вопросы:
* Введение в библиотеку threading
* Создание пула потоков для обработки блокирующего ввода-вывода
* Использование async и await для управления потоками
* Использование пулов потоков для обработки блокируюшего ввода-ввывода
* Разделяемые данные и блокировка на уровне потоков
* Обработка счётных задач с помощью потоков


Поскольку блокирующие операции ввода-вывода освобождают GIL, мы получаем возможность выполнять ввод-вывод конкурентно в разных потоках. Как и библиотека multiprocessing, asyncio позволяет использовать пулы потоков, т.е. мы можем получить все преимущества многопоточности, не отказываясь от таких API asyncio, как gather и wait.



## Введение в модуль threading

Для создания и управления потоками используется модуль threading. Он представляет класс Threa, экземпляр которого принимает функцию, которая вызывается в отдельном потоке.

Интерпретатор Python однопоточный в том смысле, что в каждый момент времени может выполняться только один участок байт-кода, даже если в процессе работает несколько потоков. GIL интерпретатора не позволяет выполнять несколько потоков одновременно.


Чтобы лучше понять, как создавать и выполнять потоки в контексте блокирующего ввода-вывода, вернёмся к примеру эхо сервера из главы 3.

Поскольку методы сокета recv и sendall - это операции ввода-вывода и освобождают GIL, ничто не мешает выполнять их в разных потоках конкурентно.
А это значит, что мы можем создать по одному потоку для каждого подключившегося клиента и в этом потоке читать и записывать данные.


Пример многопоточного эхо-сервера
```python
from threading import Thread
import socket


def echo(client: socket):
	while True:
		data = client.recv(2048)
		print(f'Получено {data}, отправляю')
		client.sendall(data)

with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server:
	server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
	server.bind(('127.0.0.1', 8000))
	server.listen()
	
	while True:
		# Блокируется в ожидании подключения клиентов
		connection, _ = server.accept()
		# Как только клиент подключился, создать поток для выполнения
		thread = Thread(target=echo, args=(connection,))
		# Начать выполнение потока
		thread.start()
	```

Здесь мы в бесконечном цикле ждём подключений к серверному сокету. Когда клиент подключился, мы создаём новый поток для выполнения функции echo. В аргументе target потоку передается функция echo, а в аргументе args кортеж аргументов. которые должны быть переданы в echo.
Затем мы запускаем поток и начинаем новую итерацию цикла, ожидая следующего подключения. Тем временем в созданном только что потоке выполняет цикл ожидания данных от клиента и эхо-ответ.

При снятии в методе server.accept() возникает исключение KeyboardInterrupt, но приложение не остановится, потому что этому мешают работающие фоновые потоки. Более того, подключенные клиенты продолжают отправлять и получать данные.

К сожалению созданные пользователем потоки не получают исключение KeyboardInterrupt, оно возбуждается только в главном потоке.

Решить эту проблему можно двумя способами:
1. Потоки демоны
2. Придумать свой способ снятия или прерывания работающего потока.

Демоны - специальный вид потоков, предназначенный для выполнения длительных фоновых задач. Они не мешают приложению завершиться. На самом деле если работают только потоки демоны, то приложение вообще завершается автоматически. Главный поток - не является демоном, но если если все потоки обслуживания подключений сделать демонами, то приложение завершиться при KeyboardInterrupt. 
Сделать поток демоном просто, нужно всего лишь написать `thread.demon = True` перед выполнением `thread.start()`.

Но у этого метода есть проблемы - потоки демоны завершаются без уведомления, и мы не можем выполнить в этот момент никакой код очистки.

Если вызвать shutdown из главного потока, то мы прервём потоки обслуживания клиентов, блокированные в вызове recv и sendall. А затем можно будет обработать исключения в клиентском потоке и выполнить очистку.
Для этого определим новый класс от Thread с методом close, в котором мы сможем остановить клиентский сокет. Тогда методы recv и sendall будут прерваны, и мы сможем остановить клиентский сокет. Тогда методы recv и sendall будут прерваны.

В классе Thread есть метод run, который можно переопределить. В подклассе Thred мы реализуем этот метод, поместив в него код, который должен выполнять поток после запуска

Пример
```python
from threading import Thread
import socket


class ClientEchoThread(Thread):
	def __init__(self, client):
		super().__init__()
		self.client = client
	
	def run(self):
		try:
			while True:
				data = self.client.recv(2048)
		
				if not data:
					raise BrokenPipeError('Подключение закрыто!')
		
				print(f'Получено {data} - отправляю')
				self.client.sendall(data)
		except OSError as e:
			print(f'Поток прерван исключением {e}, производится останговка!')
	
	def close(self):
		if self.is_alive():
			self.client.sendall(bytes('Останавливаюсь', encoding='utf-8'))
			# Разомкнуть подключение клиента, оставив чтение и запись
			self.client.shutdown(socket.SHUT_RDWR)
	
	  

with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server:
	server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
	server.bind(('127.0.0.1', 8000))	
	server.listen()
	connection_threads = []

	try:
		while True:
			connection, addr = server.accept()
			thread = ClientEchoThread(connection)
			connection_threads.append(thread)
			thread.start()

	except:
		print('Останавливаюсь')
		[thread.close() for thread in connection_threads]
```

**ВАЖНО**
Снятие потоков - задача не самая простая и зависит от конректной реализации. Нужно всегда принимать спец. меры, чтобы потоки не помешали приложению завершиться, и решить, в каком месте размещать точки прерывания для выхода из потока.



## Совместное использование потоков и asyncio

### Знакомство с исполнителями пула потоков

Как и для исполнителей пула процессов, библиотека concurent.futures содержит реализацию абстрактного класса Executor для работы с потоками - ThreadPoolExecutor. Вместо пула рабочих процессов исполнитель пула потоков создаёт и обслуживает пул потоков. которому можно передавать задания.

Если по умолчанию пул процессов по умолчанию создаёт по одному рабочему процессу для каждого имеющегося процессорного ядра, то определение количества рабочих потоков немного сложнее. По умолчанию оно равно min(32, os.cpu_count() + 4). Это значит, что **максимальное количество потоков - 32, а минимальное 5.**

Верхняя граница 32 - для избежания создания неожиданно большого числа потоков
Нижняя граница 5 - на машинах с 1-2 ядрами запуск всего двух потоков вряд ли как-то улучшит производительность.

Часто имеется смысл создавать немного больше потоков, чем имеется ядер, если предполагается использовать их для ввода-вывода.

Пример выполнения запросов при помощи пула потоков

```python
import time
import requests
from concurrent.futures import ThreadPoolExecutor


def get_status_code(url: str) -> int:
	response = requests.get(url)
	return response.status_code

start = time.time()

with ThreadPoolExecutor() as pool:
	urls = ['https://www.example.com' for _ in range(1000)]
	# pool.map() автоматически распределяет вызовы функции `get_status_code` между потоками из пула, обрабатывая несколько URL одновременно.
	results = pool.map(get_status_code, urls)
	for result in results:
		print(result)

end = time.time()

  

print(f'Запросы выполнились за {end-start:.4f} с')
```

Выполнение данного кода заняло 8-9с, тогда как синхронная версия заняла около 100с!

Однако версия в aiohttp выполнила 1000 запросов менее чем за секунду. Почему? Всё дело в накладных расходах на создания и управление потоками. Они создаются на уровне ОС и стоят дороже сопрограмм. К тому же контекстное переключение между потоками тоже имеет цену.

**При решении вопроса с количеством потоков самая оптимальная стратегия - пробовать и тестировать, находя лучшее.**


### Исполнители пула потоков и asyncio

Использование исполнителя пула потоков в цикле событий asyncio мало чем отличается от использования класса ProcessPoolExecutor. В том-то и заключается прелесть абстрактного базового класса Executor, что мы можем выполнять один и тот же код с процессами или потоками, но только изменить одну строчку.

Модифицируем пример из раздела "Знакомство с исполнителями пула потоков", заменив pool.map на asyncio.gather.

Пример
```python
import functools
import requests
import asyncio
from concurrent.futures import ThreadPoolExecutor


def get_status_code(url: str) -> int:
	response = requests.get(url)
	return response.status_code


async def main():
	loop = asyncio.get_running_loop()
	
	with ThreadPoolExecutor() as pool:
	
		urls = ['https://www.example.com' for _ in range(1000)
		# loop.run_in_executor (запускает синхронную функцию в отдельном потоке, не блокируя главный поток asyncio.)
		tasks = [
			loop.run_in_executor(
				pool, functools.partial(get_status_code, url)
			) for url in urls
		]
		results = await asyncio.gather(*tasks)
		print(results)

asyncio.run(main())
```


Под капотом метод run_in_executor вызывает метод submit исполнителя пула потоков. Это ставит все переданные задачи в очередь. Затем рабочие потоки в пуле могут выбирать задачи из очереди и выполнять до их завершения. Этот подход не даёт никакого выигрыша по сравнению с использованием пула потоков без asyncio, но когда выполняется gather и мы можем выполнять и другие задачи.


### Исполнители по умолчанию

В документации по asyncio сказано, что параметр executor метода run_in_executor может быть равен None. В этом случае используется исполнитель по умолчанию, ассоциированный с циклом событий.
Можно считать, что это допускающий повторное использование синглтонный исполнитель для всего приложения. Исполнитель по умолчанию всегда имел тип ThreadPoolExecutor, если с помощью метода loop.set_default_executor не было задано иное.
Тогда запуск из листинга выше будет выглядеть так:
```python
import functools
import requests
import asyncio


def get_status_code(url: str) -> int:
	response = requests.get(url)
	return response.status_code


async def main():
	loop = asyncio.get_running_loop()
	urls = ['https://www.example.com' for _ in range(1000)]
	tasks = [loop.run_in_executor(
		None,
		functools.partial(
			get_status_code, url
		)
	) for url in urls]
	results = await asyncio.gather(*tasks)
	print(results)
  

asyncio.run(main())
```


Здесь мы не создаём собственный экземпляр ThreadPoolExecutor для использования в качестве контекстного менеджера как раньше, а передаём в качестве исполнителя None. При первом вызове run_in_executor asyncio создаёт и кеширует исполнитель пула потоков по умолчанию.
При последующих вызовах используется уже созданный исполнитель, т.е. он оказывается глобальным относительно цикла событий.

Остановка в пуле по умолчанию происходит в момент выхода из цикла событий, а это обычно происходит при завершении приложения.

В версии 3.9 появилась сопрограмма asyncio.to_thread, которая ещё больше упрощает взаимодействие.


```python
import requests
import asyncio


def get_status_code(url: str) -> int:
	response = requests.get(url)
	return response.status_code


async def main():
	urls = ['https://www.example.com' for _ in range(1000)]
	tasks = [asyncio.to_thread(get_status_code, url) for url in urls]
	results = await asyncio.gather(*tasks)
	print(results)

asyncio.run(main())
```

## Блокировки, разделяемые данные и взаимоблокировки

Многопоточный код, как и многопроцессный, подвержен состоянию гонки при использовании разделяемых данных, потому что управлять порядком выполнения потоков мы не можем.

Потоки уже имеют доступ к памяти процесса, поэтому ничего такого делать не нужно, а к разделяемым переменным возможен прямой доступ. 

Это немного упрощает дело, но, так как мы теперь не работает с разделяемыми объектами Value, в которые блокировки уже встроены, придётся создавать их самим. Для этого воспользуемся реализацией класса Lock в модуле threading. 
Нужно лишь импортировать его и окружить критические секции вызовами его методов acquire и release, либо пометить их внутрь контекстного менеджера.

Пример
```python
import functools
import requests
import asyncio
from concurrent.futures import ThreadPoolExecutor
from threading import Lock


counter_lock = Lock()
counter: int = 0


def get_status_code(url: str) -> int:
	global counter
	response = requests.get(url)

	with counter_lock:
		counter += 1
	
	return response.status_code


async def reporter(request_count: int):
	while counter < request_count:
		print(f'Завершено запросов: {counter}/{request_count}')

		await asyncio.sleep(2)


async def main():
	loop = asyncio.get_running_loop()

	with ThreadPoolExecutor() as pool:
		request_count = 200
		urls = ['https://www.example.com' for _ in range(request_count)]
		
		reporter_task = asyncio.create_task(reporter(request_count))
		tasks = [
			loop.run_in_executor(
				pool,
				functools.partial(get_status_code, url)
			) for url in urls
		]
		
		results = asyncio.gather(*tasks)
		await reporter_task
	
	print(results)

asyncio.run(main())
```


### Реентерабельные блокировки

Простые блокировки годятся для кординации доступа к разделяемой переменной со стороны нескольких потоков, но что, если поток попытается захватить блокировку, которую сам же захватил ранее? Безопасно ли это?

Рассмотрим на примере
```python
from threading import Lock, Thread

list_lock = Lock()


def sum_list(int_list):

	print('Ожидание блокировки')
	
	with list_lock:
		print('Блокировка захвачена')
		
		if len(list_lock) == 0:
			print('Сумирование завершено')
			return
		
		else:
			head, *tail = int_list
			print('Суммируется остаток списка')
			return head + sum_list(tail)


thread = Thread(target=sum_list, args=([1,2,3,4]))
thread.start()
thread.join()
```


Вывод
```
Ожидание
Блокировка
Сумирование
Ожидание
```

Далее программа зависает навечно

Что тут происходит? В первый раз мы захватили блокировку, list_lock успешно. Затем выделили первый элемент и остаток списка и рекурсивно вызывали sum_list для остатка. При этом мы попытались во второй раз захватить list_lock. Именно в этот момент программа и зависла, попытка повторного захвата блокировки приводит к ожиданию освобождения. Но мы никогда не выйдем из блока with, потому этого никогда и не произойдёт.

Для этого и существуют реентерабельные блокировки, это специальный вид блокировки, который допускает неоднократный захват из одного потока, позволяя ему повторно входить в крит секции.

Чтобы сделать блокировку реентерабельной достаточно сделать
```python
from threading import Rlock

list_lock = RLock()
```

Теперь поток будет работать правильно и захватывать блокировку несколько раз.


Более реалистичный пример. Пусть нужно построить класс списка целых чисел, в котором имеется метод поиска всех элементов по значению и замены его другим. Этот класс будет содержать обычный список Python и блокировку для предотвращения гонки. Предположим, что существующий класс уже имеет метод indeces_of(to_find: int), который принимает целое число и возвращает индексы всех элементов списка со значением to_find.
```python
from threading import RLock

  
class IntListThreadsafe:
	def __init__(self, wrapped_list: list[int]):
		self._lock = RLock()
		self._inner_list = wrapped_list

	def indices_of(self, to_find: int) -> list[int]:
		with self._lock:
			enumerator = enumerate(self._inner_list)
			return [index for index, value in enumerator if value == to_find]

	def find_and_replace(self, to_replace: int, replace_with: int):
		with self._lock:
			indices = self.indices_of(to_replace)
			for index in indices:
				self._inner_list[index] = replace_with

  
threadsafe_list = IntListThreadsafe([1,2,1,2,1])
threadsafe_list.find_and_replace(1, 2)
print(threadsafe_list)
```

Если другой поток модифицирует список во время выполнения метода indices_of, то возможно получение неправильного значения, поэтому, перед тем как искать индексы, мы должны захватить блокировку. По той же причине должен захватить блокировку метод find_and_replace. Однако при обычной блокировке мы бы зависли, с RLock такой проблемы нет.

### Взаимоблокировки

deadlock (тупиковая ситуация) - взаимоблокировка. Ситуация, когда два потока взаимно блокируют друг друга. В таком случае RLock не поможет, потому что тут уже два потока, каждый из которых запрашивает ресурс, удерживаемый другим потоком.

![[Pasted image 20250710161854.png]]

Пример ситуации с взаимоблокировкой

```python
from threading import Lock, Thread
import time
  

a_lock = Lock()
b_lock = Lock()


def a():
	with a_lock:
	print('Захвачена блокировка а из метода а!')
	time.sleep(1)
	
	with b_lock:
		print('Захвачены обе блокировки из метода b')


def b():
	with b_lock:
		print('Захвачена блокировка b из метода b')

		with a_lock:
			print('Захвачены обе блокировки из метода b')


thread_1 = Thread(target=a)
thread_2 = Thread(target=b)

thread_1.start()
thread_2.start()
thread_1.join()
thread_2.join()
```

Сначала мы вызываем метод А и захватим блокировку А, затем вносим искуственную задержку, чтобы дать методу B захватить блокировку B. Одновременно метод B пытается захватить блокировку А, которую удерживает метод А, ожидающий пока B освободит свою блокировку. Каждый метод ждёт, пока другой освободит ресурс.


## Циклы событий в отдельных потоках

Разберём использование циклов событий в отдельных потоках на примере класса для нагрузочного тестирования.

Оно принимает URL-адрес и количество запросов. При нажатии кнопки Submit
мы будем с по­мощью aiohttp отправлять запросы с максимальной
скоростью, создавая заданную нагрузку на указанный веб-сервер. Поскольку это может занять много времени, добавим индикатор хода выполнения, чтобы видеть, далеко ли мы продвинулись. Индикатор будет обновляться после отправки 1 % запросов. Кроме того, мы предоставим пользователю возможность отменить запрос. UI будет со-держать несколько виджетов: текстовые поля для ввода тестируемого URL и количества запросов, кнопку запуска и индикатор хода выполнения.

![[Pasted image 20250710165235.png]]**Важно особенно пристально следить за состоянием гонки, потому что цикл событий asyncio не является потокобезопастным.**

Возникает искушение передавать сопрограммы из ткинтера с помощью asyncio.run, но эта функция блокирует выполнение, пока переданная сопрограмма не завершится.

Нужна функция, которая передает сопрограмму циклу событий без блокирования.
Один из вариантов - asymcio.call_soon_threadsafe. Он принимает функцию и потокобезопастным образом планирует её выполнение на следующей итерации цикла событий.

Второй - asyncio.run_coroutine_threadsafe. Она принимает сопрограмму (корутину), которая потокобезопастным образом подаёт её для выполнения и сразу возвращает будущий объект.
**Важно**. Будущий объект не asyncio, а экземпляром future из concurent.futures.

Реализуем класс нагрузочного тестирования
```python
import asyncio
from concurrent.futures import Future
from asyncio import AbstractEventLoop
from aiohttp import ClientSession
from typing import Optional, Callable


class StressTest:
	def __init__(
		self,
		loop: AbstractEventLoop,
		url: str,
		total_requests: int,
		callback: Callable[[int, int], None]
	):
		self._completed_requests: int = 0
		self._load_test_future: Optional[Future] = None
		self._loop = loop
		self._url = url
		self._total_requests = total_requests
		self._callback = callback
		self._refresh_rate = total_requests // 100

	def start(self):
		future = asyncio.run_coroutine_threadsafe(
			self._make_requests(), self._loop
		)
		self._load_test_future = future

	def cancel(self):
		if self._load_test_future:
			self._loop.call_soon_threadsafe(self._load_test_future.cancel)

	async def _get_url(self, session: ClientSession, url: str):
		try:
			await session.get(url)
		except Exception as e:
			print(e)
	
		self._completed_requests += 1
		if self._completed_requests % self._refresh_rate == 0 \
			or self._completed_requests == self._total_requests:
	
			self._callback(self._completed_requests, self._total_requests)

	async def _make_requests(self):
		async with ClientSession() as session:
			reqs = [
				self._get_url(session, self._url)
				for _ in range(self._total_requests)
			]

		await asyncio.gather(*reqs)
```


Как работает:
1. В методе start мы вызываем run_coroutine_threadsafe c `_make_requests`. Он отправляет в цикле событий запросы
2. Запоминаем будущий объект (для его отмены в cancel)
3. Создаём список сопрограмм и передаём его в gather
4. `_get_url` отправляет запрос увеличивая счётчик `_сompleted_requests` и при необходимости вызывает функции обратного вызова, передавая ей общее количество запросов и количество завершённых запросов.


**Важно** мы не ставим тут блокировку вокруг доступа к счётчику `_completed_requests`, хотя он и производится из нескольких сопрограмм. asyncio - однопоточная и цикл событий в каждый момент времени исполняет только один кусок Python-кода. Поэтому увеличение счётчика оказывается атомарной операцией внутри asyncio, хотя при выполнении из разных потоков она была бы не атомарной.

**asyncio избавляет нас от многих проблем с состоянием гонки (от многих, но не от всех)**


Простой GUI
```python
from queue import Queue
from tkinter import Tk
from tkinter import Label
from tkinter import Entry
from tkinter import ttk
from typing import Optional


class LoadTester(Tk):
	def __init__(self, loop, *args, **kwargs):
		Tk.__init__(self, *args, **kwargs)
		self._queue = Queue()
		self._refresh_ms = 25
		self._loop = loop
		self._load_test: Optional[StressTest] = None
		self.title('URL Requester')
		self._url_label = Label(self, text="URL:")
		self._url_label.grid(column=0, row=0)
		self._url_field = Entry(self, width=10)
		self._url_field.grid(column=1, row=0)
		self._request_label = Label(self, text="Number of requests:")
		self._request_label.grid(column=0, row=1)
		self._request_field = Entry(self, width=10)
		self._request_field.grid(column=1, row=1)
		self._submit = ttk.Button(self, text="Submit", command=self._start)
		self._submit.grid(column=2, row=1)
		self._pb_label = Label(self, text="Progress:")
		self._pb_label.grid(column=0, row=3)
		self._pb = ttk.Progressbar(
			self, orient="horizontal", length=200,
			mode="determinate"
		)
		self._pb.grid(column=1, row=3, columnspan=2)

	def _update_bar(self, pct: int):
		if pct == 100:
			self._load_test = None
			self._submit['text'] = 'Submit'
		else:
			self._pb['value'] = pct
			self.after(
				self._refresh_ms,
				self._poll_queue
			)
	
	def _queue_update(self, completed_requests: int, total_requests: int):
		self._queue.put(int(completed_requests / total_requests * 100))

	def _poll_queue(self):
		if not self._queue.empty():
			percent_complete = self._queue.get()
			self._update_bar(percent_complete)
		else:
			if self._load_test:
				self.after(self._refresh_ms, self._poll_queue)

	def _start(self):
		if self._load_test is None:
			self._submit['text'] = 'Cancel'
			test = StressTest(
				self._loop,
				self._url_field.get(),
				int(self._request_field.get()),
				self._queue_update
			)
			self.after(self._refresh_ms, self._poll_queue)
			test.start()
			self._load_test = test
		else:
			self._load_test.cancel()
			self._load_test = None
			self._submit['text'] = 'Submit'
```

Теперь объединим обе части приложения. Создадим новый поток для выполнения цикла событий в фоновом режиме, после чего запустим приложение LoadTesters

```python
class ThreadingEventLoop(Thread):
	def __init__(self, loop: AbstractEventLoop):
		super().__init__()
		self._loop = loop
		self.daemon = True

	def run(self):
		# Вызов run_forever() блокирует 
		# текущий поток до тех пор, пока цикл не будет остановлен.
		
		# Для неблокирующего выполнения асинхронного
		#кода параллельно с другим синхронным кодом
		
		self._loop.run_forever()

  
  

loop = asyncio.new_event_loop()

  

asyncio_thread = ThreadingEventLoop(loop)

asyncio_thread.start()

  

app = LoadTester(loop)

app.mainloop()
```



